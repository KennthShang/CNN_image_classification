{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, we'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. we'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  we'll get to apply what we learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, we'll get to see our neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CIFAR-10 Dataset: 171MB [00:20, 8.17MB/s]                              \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent our machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f78a3fa5908>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "    min_value = 0\n",
    "    max_value = 255\n",
    "    return (x-min_value)/(max_value-min_value)\n",
    "\n",
    "\n",
    "\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, we'll be implementing a function for preprocessing.  This time, we'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    one_hot = np.zeros((len(x), 10))\n",
    "    for i in range(len(x)):\n",
    "        one_hot[i][x[i]] = 1\n",
    "        \n",
    "    return one_hot\n",
    "\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Randomize Data\n",
    "As we saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but we don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Check Point\n",
    "This is our first checkpoint.  If we ever decide to come back to this notebook or have to restart the notebook, we can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Build the network\n",
    "For the neural network, we'll build each layer into a function.  Most of the code we've seen has been outside of functions. To test our code more thoroughly, we require that we put each layer in a function.  This allows us to give we better feedback and test for simple mistakes using our unittests before we submit our project.\n",
    "\n",
    ">**Note:** If we're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, we'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers we build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if we would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. we **can** still use classes from other packages that happen to have the same name as ones we find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), we would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load our saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    size = [None]\n",
    "    for item in image_shape:\n",
    "        size.append(item)\n",
    "    return tf.placeholder(tf.float32, size, name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    size = [None]\n",
    "    size.append(n_classes)\n",
    "    return tf.placeholder(tf.float32, size, name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    return tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, we should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend we use same padding, but we're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend we use same padding, but we're welcome to use any padding.\n",
    "\n",
    "**Note:** we **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but we can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. we may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    # Create the weight and bias\n",
    "    in_num = x_tensor.get_shape().as_list()[3]\n",
    "    weight = tf.Variable(tf.random_normal([conv_ksize[0], conv_ksize[1], in_num, conv_num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([conv_num_outputs]))\n",
    "    \n",
    "    # Convolution layer\n",
    "    conv = tf.nn.conv2d(x_tensor, weight, strides=[1, conv_strides[0], conv_strides[1], 1], padding=\"SAME\")\n",
    "    conv = tf.nn.bias_add(conv, bias)\n",
    "    conv = tf.nn.relu(conv)\n",
    "    \n",
    "    # Max Pooling \n",
    "    conv = tf.nn.max_pool(conv, ksize=[1, pool_ksize[0], pool_ksize[1], 1], strides=[1, pool_strides[0], pool_strides[1], 1], padding=\"SAME\")\n",
    "    return conv\n",
    "\n",
    "\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: we can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    tmp = x_tensor.get_shape().as_list()\n",
    "    flattened_size = tmp[1]*tmp[2]*tmp[3]\n",
    "    \n",
    "    flatten_layer = tf.reshape(x_tensor, [-1, flattened_size])\n",
    "    return flatten_layer\n",
    "\n",
    "\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: we can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    weight = tf.Variable(tf.random_normal([x_tensor.get_shape().as_list()[1],num_outputs], stddev=0.1))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    \n",
    "    ful = tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    ful = tf.nn.relu(ful)\n",
    "    \n",
    "    return ful\n",
    "\n",
    "\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: we can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    # Implement Function\n",
    "    weight = tf.Variable(tf.random_normal([x_tensor.get_shape().as_list()[1],num_outputs]))\n",
    "    bias = tf.Variable(tf.zeros([num_outputs]))\n",
    "    \n",
    "    \n",
    "    out = tf.nn.bias_add(tf.matmul(x_tensor, weight), bias)\n",
    "    \n",
    "    return out\n",
    "    \n",
    "\n",
    "\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers we created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    # Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    conv_1 = conv2d_maxpool(x, 32, (2 , 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv_2 = tf.nn.dropout(conv_1, keep_prob)\n",
    "    conv_2 = conv2d_maxpool(conv_1, 64, (2, 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv_2 = tf.nn.dropout(conv_2, keep_prob)\n",
    "    conv_3 = conv2d_maxpool(conv_2, 128, (2, 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv_2 = tf.nn.dropout(conv_3, keep_prob)\n",
    "    conv_4 = conv2d_maxpool(conv_3, 256, (2, 2), (1, 1), (2, 2), (2, 2))\n",
    "    conv_4 = tf.nn.dropout(conv_4, keep_prob)\n",
    "    # Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    fla = flatten(conv_4)\n",
    "\n",
    "    # Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    ful_1 = fully_conn(fla, 512)\n",
    "    ful_1 = tf.nn.dropout(ful_1, keep_prob)\n",
    "    ful_2 = fully_conn(ful_1, 256)\n",
    "    ful_2 = tf.nn.dropout(ful_2, keep_prob)\n",
    "    # Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    out = output(ful_2, 10)\n",
    "    \n",
    "    #return output\n",
    "    return out\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    #Implement Function\n",
    "    session.run(optimizer, feed_dict={x: feature_batch, y:label_batch, keep_prob:keep_probability})\n",
    "\n",
    " \n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    # Implement Function\n",
    "    loss = sess.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob:1.})\n",
    "    valid_acc = sess.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.})\n",
    "    print('Loss: {:>10.6f} Validation Accuracy: {:.6f}'.format(loss, valid_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that our machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 256\n",
    "keep_probability = 0.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while we iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:   2.301395 Validation Accuracy: 0.115000\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:   2.295889 Validation Accuracy: 0.133400\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:   2.288934 Validation Accuracy: 0.159200\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:   2.259170 Validation Accuracy: 0.154600\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:   2.208030 Validation Accuracy: 0.175000\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:   2.235615 Validation Accuracy: 0.167400\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:   2.193632 Validation Accuracy: 0.203800\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:   2.035909 Validation Accuracy: 0.261000\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:   1.976559 Validation Accuracy: 0.265200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:   1.983810 Validation Accuracy: 0.272600\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:   1.970837 Validation Accuracy: 0.304600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:   1.848413 Validation Accuracy: 0.304000\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:   1.832834 Validation Accuracy: 0.345400\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:   1.781683 Validation Accuracy: 0.332600\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:   1.691043 Validation Accuracy: 0.359000\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:   1.671722 Validation Accuracy: 0.364000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:   1.596675 Validation Accuracy: 0.398800\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:   1.649369 Validation Accuracy: 0.392600\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:   1.592385 Validation Accuracy: 0.389200\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:   1.625920 Validation Accuracy: 0.415400\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:   1.546742 Validation Accuracy: 0.416000\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:   1.494031 Validation Accuracy: 0.423000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:   1.546160 Validation Accuracy: 0.421400\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:   1.508874 Validation Accuracy: 0.436800\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:   1.525852 Validation Accuracy: 0.436000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:   1.483305 Validation Accuracy: 0.448800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:   1.367170 Validation Accuracy: 0.464000\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:   1.379876 Validation Accuracy: 0.469800\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:   1.372858 Validation Accuracy: 0.469000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:   1.285717 Validation Accuracy: 0.462800\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:   1.295533 Validation Accuracy: 0.460800\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:   1.288645 Validation Accuracy: 0.463800\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:   1.289173 Validation Accuracy: 0.471200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:   1.226985 Validation Accuracy: 0.485200\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:   1.252406 Validation Accuracy: 0.473000\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:   1.220944 Validation Accuracy: 0.483600\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:   1.139605 Validation Accuracy: 0.488800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:   1.181224 Validation Accuracy: 0.477800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:   1.107840 Validation Accuracy: 0.497800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:   1.148908 Validation Accuracy: 0.497000\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:   1.077970 Validation Accuracy: 0.508400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:   1.140211 Validation Accuracy: 0.486000\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:   1.058117 Validation Accuracy: 0.506400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:   1.003134 Validation Accuracy: 0.512400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:   0.998270 Validation Accuracy: 0.501000\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:   1.000286 Validation Accuracy: 0.512200\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:   0.926559 Validation Accuracy: 0.514000\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:   0.945151 Validation Accuracy: 0.511200\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:   0.993322 Validation Accuracy: 0.505000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:   0.927019 Validation Accuracy: 0.515000\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:   0.921770 Validation Accuracy: 0.528400\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:   0.934475 Validation Accuracy: 0.522400\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:   0.916280 Validation Accuracy: 0.522200\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:   0.896232 Validation Accuracy: 0.525600\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:   0.800488 Validation Accuracy: 0.536600\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:   0.861885 Validation Accuracy: 0.521400\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:   0.790798 Validation Accuracy: 0.534200\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:   0.773665 Validation Accuracy: 0.523200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:   0.786634 Validation Accuracy: 0.544200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:   0.818749 Validation Accuracy: 0.532600\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:   0.719784 Validation Accuracy: 0.544000\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:   0.755041 Validation Accuracy: 0.539200\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:   0.694260 Validation Accuracy: 0.556800\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:   0.717244 Validation Accuracy: 0.550200\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:   0.761811 Validation Accuracy: 0.532600\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:   0.734059 Validation Accuracy: 0.562200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:   0.667815 Validation Accuracy: 0.549400\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:   0.684148 Validation Accuracy: 0.554600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:   0.613272 Validation Accuracy: 0.568600\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:   0.661777 Validation Accuracy: 0.559800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:   0.690300 Validation Accuracy: 0.553000\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:   0.608257 Validation Accuracy: 0.553600\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:   0.552550 Validation Accuracy: 0.566200\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:   0.585826 Validation Accuracy: 0.559000\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:   0.544012 Validation Accuracy: 0.560000\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:   0.548988 Validation Accuracy: 0.567200\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:   0.522302 Validation Accuracy: 0.567800\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:   0.610183 Validation Accuracy: 0.545200\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:   0.583971 Validation Accuracy: 0.554800\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:   0.557549 Validation Accuracy: 0.558200\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:   0.589953 Validation Accuracy: 0.555800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:   0.540028 Validation Accuracy: 0.563600\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:   0.525207 Validation Accuracy: 0.555200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:   0.474181 Validation Accuracy: 0.564400\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:   0.459613 Validation Accuracy: 0.561800\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:   0.510796 Validation Accuracy: 0.570600\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:   0.464622 Validation Accuracy: 0.571000\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:   0.494396 Validation Accuracy: 0.565600\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:   0.466471 Validation Accuracy: 0.556400\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:   0.433477 Validation Accuracy: 0.581200\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:   0.452049 Validation Accuracy: 0.573800\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:   0.413350 Validation Accuracy: 0.563800\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:   0.429285 Validation Accuracy: 0.577000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:   0.423827 Validation Accuracy: 0.571800\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:   0.443846 Validation Accuracy: 0.569800\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:   0.419616 Validation Accuracy: 0.582000\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:   0.423736 Validation Accuracy: 0.563400\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:   0.402165 Validation Accuracy: 0.574000\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:   0.386230 Validation Accuracy: 0.578400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:   0.392397 Validation Accuracy: 0.574800\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:   0.341127 Validation Accuracy: 0.585600\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:   0.321033 Validation Accuracy: 0.584600\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:   0.309956 Validation Accuracy: 0.592800\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:   0.334343 Validation Accuracy: 0.585000\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:   0.317883 Validation Accuracy: 0.571800\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:   0.288639 Validation Accuracy: 0.588800\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:   0.298144 Validation Accuracy: 0.592600\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:   0.275095 Validation Accuracy: 0.593600\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:   0.280175 Validation Accuracy: 0.600400\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:   0.257584 Validation Accuracy: 0.607400\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:   0.274482 Validation Accuracy: 0.602800\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:   0.248789 Validation Accuracy: 0.608200\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:   0.226545 Validation Accuracy: 0.599600\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:   0.249646 Validation Accuracy: 0.588000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:   0.216967 Validation Accuracy: 0.597000\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:   0.217081 Validation Accuracy: 0.602800\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:   0.240255 Validation Accuracy: 0.605600\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:   0.234767 Validation Accuracy: 0.592400\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:   0.241992 Validation Accuracy: 0.614400\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:   0.208763 Validation Accuracy: 0.610800\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:   0.200942 Validation Accuracy: 0.611600\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:   0.192125 Validation Accuracy: 0.613400\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:   0.187892 Validation Accuracy: 0.615800\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:   0.205483 Validation Accuracy: 0.610400\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:   0.157151 Validation Accuracy: 0.619400\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:   0.165727 Validation Accuracy: 0.613800\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:   0.174221 Validation Accuracy: 0.626200\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:   0.163985 Validation Accuracy: 0.607800\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:   0.150924 Validation Accuracy: 0.616600\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:   0.156590 Validation Accuracy: 0.626000\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:   0.149343 Validation Accuracy: 0.627000\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:   0.140245 Validation Accuracy: 0.624000\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:   0.182661 Validation Accuracy: 0.622400\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:   0.150052 Validation Accuracy: 0.626600\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:   0.143578 Validation Accuracy: 0.615000\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:   0.146982 Validation Accuracy: 0.613600\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:   0.152193 Validation Accuracy: 0.624200\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:   0.127391 Validation Accuracy: 0.622600\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:   0.127920 Validation Accuracy: 0.617200\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:   0.106882 Validation Accuracy: 0.624200\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:   0.124309 Validation Accuracy: 0.623800\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:   0.126773 Validation Accuracy: 0.606400\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:   0.117775 Validation Accuracy: 0.627400\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:   0.114867 Validation Accuracy: 0.613000\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:   0.127151 Validation Accuracy: 0.605200\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:   0.107400 Validation Accuracy: 0.620800\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:   0.113793 Validation Accuracy: 0.630600\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:   0.096328 Validation Accuracy: 0.624000\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:   0.099679 Validation Accuracy: 0.624000\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:   0.104328 Validation Accuracy: 0.636800\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:   0.095058 Validation Accuracy: 0.638200\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:   0.113205 Validation Accuracy: 0.623000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:   0.104499 Validation Accuracy: 0.623000\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:   0.076776 Validation Accuracy: 0.628800\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:   0.089807 Validation Accuracy: 0.617600\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:   0.087654 Validation Accuracy: 0.633200\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:   0.065083 Validation Accuracy: 0.628400\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:   0.089181 Validation Accuracy: 0.627400\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:   0.093121 Validation Accuracy: 0.619400\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:   0.082541 Validation Accuracy: 0.614200\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:   0.054997 Validation Accuracy: 0.635000\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:   0.062100 Validation Accuracy: 0.638200\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:   0.062410 Validation Accuracy: 0.624000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:   0.077873 Validation Accuracy: 0.621800\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:   0.075760 Validation Accuracy: 0.608200\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:   0.095361 Validation Accuracy: 0.622600\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:   0.070582 Validation Accuracy: 0.622800\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:   0.055300 Validation Accuracy: 0.630000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:   0.056730 Validation Accuracy: 0.634800\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:   0.062019 Validation Accuracy: 0.624400\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:   0.060220 Validation Accuracy: 0.636400\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:   0.063752 Validation Accuracy: 0.628800\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:   0.048033 Validation Accuracy: 0.637400\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:   0.063573 Validation Accuracy: 0.641800\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:   0.050918 Validation Accuracy: 0.626000\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:   0.049201 Validation Accuracy: 0.619800\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:   0.038264 Validation Accuracy: 0.643400\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:   0.043961 Validation Accuracy: 0.624800\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:   0.041356 Validation Accuracy: 0.626400\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:   0.036210 Validation Accuracy: 0.628400\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:   0.062101 Validation Accuracy: 0.619000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:   0.041345 Validation Accuracy: 0.647400\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:   0.039506 Validation Accuracy: 0.636000\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:   0.032052 Validation Accuracy: 0.627200\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:   0.031377 Validation Accuracy: 0.636800\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:   0.040120 Validation Accuracy: 0.639400\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:   0.028563 Validation Accuracy: 0.653600\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:   0.031661 Validation Accuracy: 0.641800\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:   0.035684 Validation Accuracy: 0.646000\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:   0.047647 Validation Accuracy: 0.641000\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:   0.038668 Validation Accuracy: 0.629200\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:   0.030665 Validation Accuracy: 0.643200\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:   0.028242 Validation Accuracy: 0.642200\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:   0.026240 Validation Accuracy: 0.641600\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:   0.025791 Validation Accuracy: 0.637800\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:   0.032571 Validation Accuracy: 0.647000\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:   0.025363 Validation Accuracy: 0.647800\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:   0.026442 Validation Accuracy: 0.639200\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:   0.023837 Validation Accuracy: 0.651600\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:   0.020603 Validation Accuracy: 0.643200\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Fully Train the Model\n",
    "Now that we got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss:   2.298740 Validation Accuracy: 0.118800\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss:   2.286209 Validation Accuracy: 0.164200\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss:   2.204384 Validation Accuracy: 0.154800\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss:   2.146364 Validation Accuracy: 0.188200\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss:   2.144347 Validation Accuracy: 0.212800\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss:   2.125990 Validation Accuracy: 0.246400\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss:   1.927050 Validation Accuracy: 0.274000\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss:   1.845220 Validation Accuracy: 0.286800\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss:   1.866186 Validation Accuracy: 0.319800\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss:   1.913692 Validation Accuracy: 0.316600\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss:   1.941296 Validation Accuracy: 0.334200\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss:   1.808179 Validation Accuracy: 0.324000\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss:   1.674932 Validation Accuracy: 0.353000\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss:   1.646459 Validation Accuracy: 0.385800\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss:   1.840799 Validation Accuracy: 0.359800\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss:   1.765956 Validation Accuracy: 0.395200\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss:   1.651650 Validation Accuracy: 0.396800\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss:   1.501561 Validation Accuracy: 0.417200\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss:   1.535925 Validation Accuracy: 0.422000\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss:   1.675281 Validation Accuracy: 0.397800\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss:   1.664558 Validation Accuracy: 0.446600\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss:   1.558587 Validation Accuracy: 0.422400\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss:   1.438014 Validation Accuracy: 0.435000\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss:   1.438266 Validation Accuracy: 0.456200\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss:   1.482113 Validation Accuracy: 0.467200\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss:   1.595732 Validation Accuracy: 0.472200\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss:   1.525498 Validation Accuracy: 0.450200\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss:   1.388073 Validation Accuracy: 0.456800\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss:   1.423293 Validation Accuracy: 0.480000\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss:   1.407086 Validation Accuracy: 0.473600\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss:   1.564598 Validation Accuracy: 0.490200\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss:   1.475832 Validation Accuracy: 0.467200\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss:   1.280488 Validation Accuracy: 0.485400\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss:   1.379571 Validation Accuracy: 0.483200\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss:   1.441924 Validation Accuracy: 0.493400\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss:   1.495017 Validation Accuracy: 0.522800\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss:   1.354584 Validation Accuracy: 0.496000\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss:   1.291578 Validation Accuracy: 0.470400\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss:   1.289926 Validation Accuracy: 0.515200\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss:   1.319232 Validation Accuracy: 0.500600\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss:   1.480326 Validation Accuracy: 0.522400\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss:   1.269915 Validation Accuracy: 0.512400\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss:   1.095376 Validation Accuracy: 0.510600\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss:   1.231303 Validation Accuracy: 0.515800\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss:   1.291151 Validation Accuracy: 0.513200\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss:   1.444610 Validation Accuracy: 0.526200\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss:   1.265846 Validation Accuracy: 0.516600\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss:   1.081003 Validation Accuracy: 0.530800\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss:   1.163926 Validation Accuracy: 0.548200\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss:   1.320299 Validation Accuracy: 0.513000\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss:   1.390297 Validation Accuracy: 0.548400\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss:   1.227054 Validation Accuracy: 0.533000\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss:   1.030915 Validation Accuracy: 0.524400\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss:   1.156252 Validation Accuracy: 0.552800\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss:   1.219006 Validation Accuracy: 0.515600\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss:   1.326011 Validation Accuracy: 0.560000\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss:   1.152123 Validation Accuracy: 0.549200\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss:   0.983583 Validation Accuracy: 0.551800\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss:   1.155755 Validation Accuracy: 0.554600\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss:   1.174950 Validation Accuracy: 0.552200\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss:   1.263962 Validation Accuracy: 0.555400\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss:   1.127124 Validation Accuracy: 0.561600\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss:   0.984072 Validation Accuracy: 0.562600\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss:   1.095931 Validation Accuracy: 0.554800\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss:   1.171942 Validation Accuracy: 0.551600\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss:   1.279717 Validation Accuracy: 0.583000\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss:   1.113903 Validation Accuracy: 0.575000\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss:   0.931740 Validation Accuracy: 0.571800\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss:   1.041242 Validation Accuracy: 0.580000\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss:   1.069312 Validation Accuracy: 0.564000\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss:   1.277566 Validation Accuracy: 0.565600\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss:   1.037537 Validation Accuracy: 0.581600\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss:   0.894537 Validation Accuracy: 0.575600\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss:   1.027159 Validation Accuracy: 0.572400\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss:   1.052972 Validation Accuracy: 0.570400\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss:   1.129595 Validation Accuracy: 0.593200\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss:   1.043041 Validation Accuracy: 0.589000\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss:   0.866149 Validation Accuracy: 0.582200\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss:   0.961359 Validation Accuracy: 0.604000\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss:   0.982778 Validation Accuracy: 0.600000\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss:   1.057265 Validation Accuracy: 0.609200\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss:   1.064460 Validation Accuracy: 0.597800\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss:   0.875492 Validation Accuracy: 0.599800\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss:   0.935858 Validation Accuracy: 0.610000\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss:   0.996991 Validation Accuracy: 0.605000\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss:   1.027692 Validation Accuracy: 0.617200\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss:   0.952742 Validation Accuracy: 0.597800\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss:   0.781407 Validation Accuracy: 0.607800\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss:   0.922516 Validation Accuracy: 0.614000\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss:   0.953786 Validation Accuracy: 0.617000\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss:   0.974738 Validation Accuracy: 0.603200\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss:   1.018496 Validation Accuracy: 0.595600\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss:   0.730201 Validation Accuracy: 0.614000\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss:   0.949272 Validation Accuracy: 0.602000\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss:   0.872242 Validation Accuracy: 0.615400\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss:   0.948465 Validation Accuracy: 0.608600\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss:   0.959193 Validation Accuracy: 0.625200\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss:   0.696054 Validation Accuracy: 0.625200\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss:   0.774214 Validation Accuracy: 0.628800\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss:   0.823591 Validation Accuracy: 0.628000\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss:   0.921802 Validation Accuracy: 0.634400\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss:   0.902907 Validation Accuracy: 0.610000\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss:   0.708409 Validation Accuracy: 0.623200\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss:   0.816915 Validation Accuracy: 0.628800\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss:   0.912783 Validation Accuracy: 0.609400\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss:   0.841257 Validation Accuracy: 0.625800\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss:   0.837647 Validation Accuracy: 0.626800\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss:   0.708621 Validation Accuracy: 0.632000\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss:   0.733220 Validation Accuracy: 0.647800\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss:   0.746822 Validation Accuracy: 0.642000\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss:   0.841999 Validation Accuracy: 0.641400\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss:   0.804365 Validation Accuracy: 0.636200\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss:   0.636659 Validation Accuracy: 0.646200\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss:   0.710353 Validation Accuracy: 0.648000\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss:   0.739731 Validation Accuracy: 0.641000\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss:   0.784994 Validation Accuracy: 0.645000\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss:   0.734649 Validation Accuracy: 0.629800\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss:   0.616328 Validation Accuracy: 0.652000\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss:   0.696011 Validation Accuracy: 0.649600\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss:   0.723681 Validation Accuracy: 0.654400\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss:   0.748202 Validation Accuracy: 0.645000\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss:   0.705466 Validation Accuracy: 0.641000\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss:   0.569891 Validation Accuracy: 0.643600\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss:   0.671352 Validation Accuracy: 0.654800\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss:   0.773142 Validation Accuracy: 0.639000\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss:   0.770647 Validation Accuracy: 0.628400\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss:   0.710982 Validation Accuracy: 0.655600\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss:   0.550990 Validation Accuracy: 0.655800\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss:   0.633038 Validation Accuracy: 0.658400\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss:   0.639097 Validation Accuracy: 0.658800\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss:   0.715548 Validation Accuracy: 0.640800\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss:   0.760801 Validation Accuracy: 0.638000\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss:   0.525706 Validation Accuracy: 0.665400\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss:   0.601790 Validation Accuracy: 0.660600\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss:   0.625734 Validation Accuracy: 0.649600\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss:   0.629775 Validation Accuracy: 0.652400\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss:   0.602247 Validation Accuracy: 0.653200\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss:   0.558937 Validation Accuracy: 0.673000\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss:   0.588581 Validation Accuracy: 0.662400\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss:   0.584195 Validation Accuracy: 0.675600\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss:   0.611267 Validation Accuracy: 0.656400\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss:   0.649437 Validation Accuracy: 0.673800\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss:   0.503897 Validation Accuracy: 0.678200\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss:   0.578818 Validation Accuracy: 0.661600\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss:   0.587170 Validation Accuracy: 0.656000\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss:   0.669954 Validation Accuracy: 0.646000\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss:   0.580401 Validation Accuracy: 0.664800\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss:   0.508193 Validation Accuracy: 0.676400\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss:   0.612794 Validation Accuracy: 0.677800\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss:   0.560719 Validation Accuracy: 0.680000\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss:   0.600961 Validation Accuracy: 0.660400\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss:   0.567490 Validation Accuracy: 0.663000\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss:   0.509833 Validation Accuracy: 0.676600\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss:   0.568616 Validation Accuracy: 0.680000\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss:   0.580477 Validation Accuracy: 0.671600\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss:   0.583930 Validation Accuracy: 0.681000\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss:   0.557229 Validation Accuracy: 0.665400\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss:   0.539146 Validation Accuracy: 0.676400\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss:   0.569673 Validation Accuracy: 0.673000\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss:   0.528348 Validation Accuracy: 0.684200\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss:   0.575497 Validation Accuracy: 0.693400\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss:   0.610088 Validation Accuracy: 0.669600\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss:   0.464503 Validation Accuracy: 0.681600\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss:   0.632219 Validation Accuracy: 0.666800\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss:   0.588268 Validation Accuracy: 0.672200\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss:   0.543615 Validation Accuracy: 0.680800\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss:   0.609511 Validation Accuracy: 0.663200\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss:   0.416256 Validation Accuracy: 0.692800\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss:   0.492723 Validation Accuracy: 0.684200\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss:   0.528200 Validation Accuracy: 0.683000\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss:   0.505471 Validation Accuracy: 0.681600\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss:   0.515217 Validation Accuracy: 0.685200\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss:   0.455208 Validation Accuracy: 0.688000\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss:   0.469111 Validation Accuracy: 0.691600\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss:   0.538311 Validation Accuracy: 0.692800\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss:   0.510067 Validation Accuracy: 0.694200\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss:   0.561127 Validation Accuracy: 0.687200\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss:   0.437488 Validation Accuracy: 0.692600\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss:   0.435450 Validation Accuracy: 0.706600\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss:   0.514537 Validation Accuracy: 0.694000\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss:   0.505336 Validation Accuracy: 0.686600\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss:   0.476758 Validation Accuracy: 0.692400\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss:   0.404339 Validation Accuracy: 0.695400\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss:   0.470112 Validation Accuracy: 0.691600\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss:   0.462018 Validation Accuracy: 0.691800\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss:   0.514095 Validation Accuracy: 0.684200\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss:   0.532428 Validation Accuracy: 0.694800\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss:   0.476358 Validation Accuracy: 0.677800\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss:   0.428599 Validation Accuracy: 0.701200\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss:   0.431178 Validation Accuracy: 0.705800\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss:   0.476248 Validation Accuracy: 0.700400\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss:   0.457544 Validation Accuracy: 0.688200\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss:   0.372145 Validation Accuracy: 0.697800\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss:   0.398270 Validation Accuracy: 0.708600\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss:   0.432017 Validation Accuracy: 0.696800\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss:   0.436124 Validation Accuracy: 0.696800\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss:   0.445007 Validation Accuracy: 0.702800\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss:   0.411250 Validation Accuracy: 0.694200\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss:   0.477116 Validation Accuracy: 0.685800\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss:   0.447338 Validation Accuracy: 0.711800\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss:   0.375544 Validation Accuracy: 0.710400\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss:   0.475544 Validation Accuracy: 0.686600\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss:   0.341444 Validation Accuracy: 0.702800\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss:   0.432060 Validation Accuracy: 0.691200\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss:   0.414025 Validation Accuracy: 0.707400\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss:   0.419376 Validation Accuracy: 0.718600\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss:   0.446110 Validation Accuracy: 0.711600\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss:   0.340764 Validation Accuracy: 0.705000\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss:   0.447465 Validation Accuracy: 0.691000\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss:   0.411507 Validation Accuracy: 0.698800\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss:   0.473799 Validation Accuracy: 0.697000\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss:   0.410642 Validation Accuracy: 0.707400\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss:   0.418318 Validation Accuracy: 0.695800\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss:   0.411551 Validation Accuracy: 0.698200\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss:   0.415499 Validation Accuracy: 0.709400\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss:   0.450515 Validation Accuracy: 0.719000\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss:   0.444213 Validation Accuracy: 0.707000\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss:   0.327410 Validation Accuracy: 0.717200\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss:   0.316860 Validation Accuracy: 0.732000\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss:   0.396445 Validation Accuracy: 0.719400\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss:   0.403756 Validation Accuracy: 0.703600\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss:   0.402408 Validation Accuracy: 0.706600\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss:   0.351666 Validation Accuracy: 0.706200\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss:   0.365549 Validation Accuracy: 0.721200\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss:   0.391834 Validation Accuracy: 0.712400\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss:   0.370496 Validation Accuracy: 0.719200\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss:   0.407954 Validation Accuracy: 0.729400\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss:   0.336089 Validation Accuracy: 0.718400\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss:   0.330940 Validation Accuracy: 0.732200\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss:   0.430260 Validation Accuracy: 0.714400\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss:   0.364125 Validation Accuracy: 0.723400\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss:   0.432459 Validation Accuracy: 0.717800\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss:   0.304424 Validation Accuracy: 0.709600\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss:   0.333917 Validation Accuracy: 0.732400\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss:   0.351700 Validation Accuracy: 0.726800\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss:   0.346747 Validation Accuracy: 0.719200\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss:   0.358387 Validation Accuracy: 0.719600\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss:   0.309626 Validation Accuracy: 0.714000\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss:   0.339210 Validation Accuracy: 0.726600\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss:   0.333411 Validation Accuracy: 0.728800\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss:   0.312499 Validation Accuracy: 0.733800\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss:   0.352688 Validation Accuracy: 0.725200\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss:   0.267777 Validation Accuracy: 0.719600\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss:   0.332395 Validation Accuracy: 0.717600\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss:   0.329817 Validation Accuracy: 0.724000\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss:   0.351548 Validation Accuracy: 0.734800\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss:   0.352492 Validation Accuracy: 0.727800\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss:   0.247178 Validation Accuracy: 0.727800\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss:   0.314054 Validation Accuracy: 0.734200\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss:   0.332574 Validation Accuracy: 0.714600\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss:   0.320608 Validation Accuracy: 0.729800\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss:   0.295973 Validation Accuracy: 0.721000\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss:   0.330181 Validation Accuracy: 0.687800\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss:   0.356857 Validation Accuracy: 0.732000\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss:   0.355334 Validation Accuracy: 0.727800\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss:   0.291832 Validation Accuracy: 0.733000\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss:   0.298846 Validation Accuracy: 0.726400\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss:   0.291111 Validation Accuracy: 0.708400\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss:   0.279740 Validation Accuracy: 0.744000\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss:   0.328604 Validation Accuracy: 0.722600\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss:   0.305870 Validation Accuracy: 0.740600\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss:   0.312555 Validation Accuracy: 0.732800\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss:   0.257357 Validation Accuracy: 0.726800\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss:   0.311204 Validation Accuracy: 0.731000\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss:   0.305791 Validation Accuracy: 0.729000\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss:   0.291711 Validation Accuracy: 0.736400\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss:   0.332631 Validation Accuracy: 0.714400\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss:   0.275478 Validation Accuracy: 0.715600\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss:   0.268490 Validation Accuracy: 0.742800\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss:   0.290432 Validation Accuracy: 0.731000\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss:   0.285228 Validation Accuracy: 0.746800\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss:   0.307507 Validation Accuracy: 0.730400\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss:   0.268410 Validation Accuracy: 0.700400\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss:   0.266429 Validation Accuracy: 0.735400\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss:   0.257885 Validation Accuracy: 0.733400\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss:   0.272334 Validation Accuracy: 0.744400\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss:   0.271312 Validation Accuracy: 0.716200\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss:   0.252324 Validation Accuracy: 0.713200\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss:   0.239498 Validation Accuracy: 0.743200\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss:   0.240774 Validation Accuracy: 0.743200\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss:   0.270417 Validation Accuracy: 0.742400\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss:   0.288571 Validation Accuracy: 0.732000\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss:   0.231062 Validation Accuracy: 0.727600\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss:   0.271774 Validation Accuracy: 0.744400\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss:   0.265176 Validation Accuracy: 0.733400\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss:   0.252501 Validation Accuracy: 0.739800\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss:   0.282466 Validation Accuracy: 0.733200\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss:   0.211175 Validation Accuracy: 0.727800\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss:   0.225026 Validation Accuracy: 0.749800\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss:   0.250170 Validation Accuracy: 0.744200\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss:   0.245825 Validation Accuracy: 0.743200\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss:   0.237808 Validation Accuracy: 0.747200\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss:   0.178447 Validation Accuracy: 0.738400\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss:   0.278378 Validation Accuracy: 0.734600\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss:   0.218931 Validation Accuracy: 0.744200\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss:   0.263467 Validation Accuracy: 0.742200\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss:   0.248645 Validation Accuracy: 0.740400\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss:   0.191643 Validation Accuracy: 0.736200\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss:   0.243039 Validation Accuracy: 0.741400\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss:   0.217511 Validation Accuracy: 0.744200\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss:   0.257180 Validation Accuracy: 0.735600\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss:   0.237635 Validation Accuracy: 0.731400\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss:   0.225913 Validation Accuracy: 0.726000\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss:   0.272981 Validation Accuracy: 0.732600\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss:   0.257541 Validation Accuracy: 0.745400\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss:   0.247716 Validation Accuracy: 0.733800\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss:   0.235070 Validation Accuracy: 0.737400\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss:   0.178701 Validation Accuracy: 0.746200\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss:   0.190854 Validation Accuracy: 0.750800\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss:   0.223019 Validation Accuracy: 0.744600\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss:   0.215034 Validation Accuracy: 0.755400\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss:   0.279158 Validation Accuracy: 0.732200\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss:   0.239160 Validation Accuracy: 0.733000\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss:   0.197945 Validation Accuracy: 0.749200\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss:   0.229024 Validation Accuracy: 0.751200\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss:   0.228109 Validation Accuracy: 0.756800\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss:   0.250288 Validation Accuracy: 0.749000\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss:   0.179023 Validation Accuracy: 0.741000\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss:   0.180447 Validation Accuracy: 0.749200\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss:   0.230693 Validation Accuracy: 0.746000\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss:   0.214972 Validation Accuracy: 0.753200\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss:   0.204878 Validation Accuracy: 0.748400\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss:   0.174656 Validation Accuracy: 0.750200\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss:   0.196224 Validation Accuracy: 0.754600\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss:   0.270082 Validation Accuracy: 0.731200\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss:   0.202183 Validation Accuracy: 0.756000\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss:   0.213413 Validation Accuracy: 0.743600\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss:   0.198197 Validation Accuracy: 0.729200\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss:   0.198063 Validation Accuracy: 0.746800\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss:   0.213103 Validation Accuracy: 0.736200\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss:   0.232072 Validation Accuracy: 0.747400\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss:   0.197607 Validation Accuracy: 0.751400\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss:   0.213039 Validation Accuracy: 0.732200\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss:   0.188178 Validation Accuracy: 0.751000\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss:   0.215165 Validation Accuracy: 0.747800\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss:   0.195133 Validation Accuracy: 0.755200\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss:   0.226539 Validation Accuracy: 0.728200\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss:   0.179959 Validation Accuracy: 0.744000\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss:   0.194328 Validation Accuracy: 0.752400\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss:   0.179662 Validation Accuracy: 0.749600\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss:   0.200103 Validation Accuracy: 0.752400\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss:   0.214857 Validation Accuracy: 0.738800\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss:   0.171699 Validation Accuracy: 0.731600\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss:   0.195710 Validation Accuracy: 0.746000\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss:   0.172631 Validation Accuracy: 0.737000\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss:   0.169498 Validation Accuracy: 0.749000\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss:   0.170047 Validation Accuracy: 0.734800\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss:   0.186052 Validation Accuracy: 0.731600\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss:   0.177020 Validation Accuracy: 0.752800\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss:   0.159927 Validation Accuracy: 0.754800\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss:   0.169249 Validation Accuracy: 0.754800\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss:   0.206665 Validation Accuracy: 0.759800\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss:   0.165227 Validation Accuracy: 0.749400\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss:   0.163278 Validation Accuracy: 0.756800\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss:   0.177634 Validation Accuracy: 0.754600\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss:   0.163694 Validation Accuracy: 0.758400\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss:   0.212370 Validation Accuracy: 0.741000\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss:   0.148588 Validation Accuracy: 0.749800\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss:   0.157279 Validation Accuracy: 0.760400\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss:   0.133076 Validation Accuracy: 0.759800\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss:   0.158658 Validation Accuracy: 0.754400\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss:   0.179864 Validation Accuracy: 0.755000\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss:   0.155797 Validation Accuracy: 0.744600\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss:   0.141806 Validation Accuracy: 0.762000\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss:   0.151251 Validation Accuracy: 0.742800\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss:   0.166651 Validation Accuracy: 0.762000\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss:   0.181060 Validation Accuracy: 0.755800\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss:   0.160114 Validation Accuracy: 0.765800\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss:   0.154840 Validation Accuracy: 0.755600\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss:   0.135606 Validation Accuracy: 0.752200\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss:   0.167348 Validation Accuracy: 0.757800\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss:   0.174721 Validation Accuracy: 0.750800\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss:   0.159289 Validation Accuracy: 0.745600\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss:   0.152666 Validation Accuracy: 0.759200\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss:   0.121335 Validation Accuracy: 0.747200\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss:   0.166570 Validation Accuracy: 0.758400\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss:   0.236564 Validation Accuracy: 0.746400\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss:   0.142483 Validation Accuracy: 0.752400\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss:   0.185289 Validation Accuracy: 0.744200\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss:   0.130640 Validation Accuracy: 0.750600\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss:   0.161727 Validation Accuracy: 0.753000\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss:   0.211989 Validation Accuracy: 0.744200\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss:   0.150084 Validation Accuracy: 0.742600\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss:   0.136353 Validation Accuracy: 0.761400\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss:   0.156060 Validation Accuracy: 0.743200\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss:   0.147900 Validation Accuracy: 0.760000\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss:   0.198263 Validation Accuracy: 0.758400\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss:   0.153183 Validation Accuracy: 0.760200\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss:   0.126323 Validation Accuracy: 0.762800\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss:   0.135039 Validation Accuracy: 0.759800\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss:   0.155967 Validation Accuracy: 0.763000\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss:   0.182221 Validation Accuracy: 0.747200\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss:   0.156763 Validation Accuracy: 0.750600\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss:   0.127878 Validation Accuracy: 0.763000\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss:   0.164419 Validation Accuracy: 0.756000\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss:   0.146343 Validation Accuracy: 0.759800\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss:   0.209008 Validation Accuracy: 0.752000\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss:   0.138172 Validation Accuracy: 0.747200\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss:   0.122193 Validation Accuracy: 0.756000\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss:   0.115956 Validation Accuracy: 0.755400\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss:   0.150318 Validation Accuracy: 0.756600\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss:   0.187432 Validation Accuracy: 0.760600\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss:   0.123290 Validation Accuracy: 0.756600\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss:   0.109316 Validation Accuracy: 0.764400\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss:   0.160502 Validation Accuracy: 0.748800\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss:   0.150511 Validation Accuracy: 0.764600\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss:   0.198523 Validation Accuracy: 0.758600\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss:   0.125986 Validation Accuracy: 0.750200\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss:   0.124879 Validation Accuracy: 0.763400\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss:   0.130105 Validation Accuracy: 0.752000\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss:   0.134840 Validation Accuracy: 0.762600\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss:   0.162317 Validation Accuracy: 0.744600\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss:   0.137973 Validation Accuracy: 0.754400\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss:   0.124949 Validation Accuracy: 0.762000\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss:   0.130539 Validation Accuracy: 0.763200\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss:   0.165660 Validation Accuracy: 0.760400\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss:   0.164804 Validation Accuracy: 0.762600\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss:   0.123779 Validation Accuracy: 0.759400\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss:   0.132022 Validation Accuracy: 0.757600\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss:   0.129008 Validation Accuracy: 0.755000\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss:   0.126985 Validation Accuracy: 0.759600\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss:   0.145336 Validation Accuracy: 0.759200\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss:   0.132137 Validation Accuracy: 0.752400\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss:   0.127845 Validation Accuracy: 0.763800\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss:   0.125171 Validation Accuracy: 0.760400\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss:   0.124305 Validation Accuracy: 0.763400\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss:   0.146534 Validation Accuracy: 0.769600\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss:   0.120396 Validation Accuracy: 0.754800\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss:   0.099457 Validation Accuracy: 0.766600\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss:   0.114952 Validation Accuracy: 0.763000\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss:   0.138428 Validation Accuracy: 0.771800\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss:   0.151727 Validation Accuracy: 0.743000\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss:   0.134329 Validation Accuracy: 0.750200\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss:   0.113407 Validation Accuracy: 0.762000\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss:   0.115801 Validation Accuracy: 0.762200\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss:   0.127421 Validation Accuracy: 0.776600\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss:   0.147137 Validation Accuracy: 0.746000\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss:   0.137291 Validation Accuracy: 0.759200\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss:   0.103746 Validation Accuracy: 0.771400\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss:   0.097084 Validation Accuracy: 0.775800\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss:   0.123990 Validation Accuracy: 0.775400\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss:   0.128740 Validation Accuracy: 0.752600\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss:   0.120231 Validation Accuracy: 0.758600\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss:   0.109535 Validation Accuracy: 0.753000\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss:   0.132178 Validation Accuracy: 0.762200\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss:   0.138085 Validation Accuracy: 0.768000\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss:   0.124397 Validation Accuracy: 0.759400\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss:   0.113872 Validation Accuracy: 0.761400\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss:   0.105946 Validation Accuracy: 0.776000\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss:   0.114410 Validation Accuracy: 0.768000\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss:   0.111056 Validation Accuracy: 0.771000\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss:   0.100874 Validation Accuracy: 0.769200\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss:   0.112785 Validation Accuracy: 0.766400\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss:   0.111012 Validation Accuracy: 0.766000\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss:   0.127902 Validation Accuracy: 0.757400\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss:   0.098980 Validation Accuracy: 0.770400\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss:   0.122690 Validation Accuracy: 0.766000\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss:   0.114490 Validation Accuracy: 0.746400\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss:   0.115392 Validation Accuracy: 0.770000\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss:   0.088261 Validation Accuracy: 0.768600\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss:   0.118575 Validation Accuracy: 0.760600\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss:   0.145193 Validation Accuracy: 0.755200\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss:   0.095789 Validation Accuracy: 0.763000\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss:   0.107985 Validation Accuracy: 0.771800\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss:   0.092008 Validation Accuracy: 0.766000\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss:   0.115725 Validation Accuracy: 0.770400\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss:   0.145510 Validation Accuracy: 0.758200\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss:   0.112034 Validation Accuracy: 0.768200\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss:   0.116782 Validation Accuracy: 0.762400\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss:   0.116398 Validation Accuracy: 0.752600\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss:   0.106927 Validation Accuracy: 0.775200\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss:   0.118020 Validation Accuracy: 0.764000\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss:   0.101617 Validation Accuracy: 0.759400\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss:   0.084216 Validation Accuracy: 0.768000\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss:   0.103062 Validation Accuracy: 0.754800\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss:   0.139267 Validation Accuracy: 0.768400\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss:   0.164428 Validation Accuracy: 0.746600\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss:   0.090962 Validation Accuracy: 0.764800\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss:   0.077390 Validation Accuracy: 0.768200\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss:   0.094200 Validation Accuracy: 0.774400\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss:   0.094419 Validation Accuracy: 0.768600\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss:   0.144057 Validation Accuracy: 0.761200\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss:   0.091566 Validation Accuracy: 0.759200\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss:   0.070461 Validation Accuracy: 0.769800\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss:   0.089912 Validation Accuracy: 0.758600\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss:   0.104387 Validation Accuracy: 0.769400\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss:   0.137177 Validation Accuracy: 0.761800\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss:   0.092082 Validation Accuracy: 0.773200\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss:   0.088481 Validation Accuracy: 0.776800\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss:   0.094932 Validation Accuracy: 0.770600\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss:   0.118357 Validation Accuracy: 0.770400\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss:   0.121522 Validation Accuracy: 0.773600\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss:   0.089843 Validation Accuracy: 0.759800\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss:   0.098452 Validation Accuracy: 0.762800\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss:   0.073013 Validation Accuracy: 0.768400\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss:   0.098724 Validation Accuracy: 0.776200\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss:   0.108678 Validation Accuracy: 0.764200\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss:   0.100215 Validation Accuracy: 0.775600\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss:   0.085206 Validation Accuracy: 0.778400\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss:   0.085405 Validation Accuracy: 0.775000\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss:   0.113514 Validation Accuracy: 0.761000\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss:   0.117219 Validation Accuracy: 0.770200\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss:   0.089113 Validation Accuracy: 0.772200\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss:   0.081600 Validation Accuracy: 0.771200\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss:   0.087765 Validation Accuracy: 0.771800\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss:   0.117545 Validation Accuracy: 0.767200\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss:   0.129781 Validation Accuracy: 0.762400\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss:   0.076421 Validation Accuracy: 0.763600\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss:   0.078901 Validation Accuracy: 0.773800\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss:   0.070095 Validation Accuracy: 0.774000\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss:   0.104599 Validation Accuracy: 0.773400\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss:   0.101986 Validation Accuracy: 0.764600\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss:   0.099355 Validation Accuracy: 0.765200\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss:   0.074653 Validation Accuracy: 0.779000\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss:   0.071308 Validation Accuracy: 0.770200\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss:   0.104149 Validation Accuracy: 0.780800\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss:   0.119482 Validation Accuracy: 0.765200\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss:   0.086329 Validation Accuracy: 0.755400\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss:   0.086365 Validation Accuracy: 0.766200\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss:   0.075659 Validation Accuracy: 0.771400\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss:   0.107511 Validation Accuracy: 0.766800\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss:   0.105159 Validation Accuracy: 0.769800\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss:   0.084731 Validation Accuracy: 0.768800\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss:   0.078078 Validation Accuracy: 0.768800\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss:   0.064565 Validation Accuracy: 0.772000\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss:   0.088236 Validation Accuracy: 0.774000\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss:   0.106447 Validation Accuracy: 0.777600\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss:   0.082235 Validation Accuracy: 0.763800\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss:   0.065478 Validation Accuracy: 0.775200\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss:   0.063355 Validation Accuracy: 0.777000\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss:   0.097822 Validation Accuracy: 0.775000\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss:   0.107235 Validation Accuracy: 0.776400\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss:   0.075779 Validation Accuracy: 0.768200\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss:   0.074383 Validation Accuracy: 0.775800\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss:   0.074580 Validation Accuracy: 0.772000\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss:   0.099095 Validation Accuracy: 0.771600\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss:   0.097889 Validation Accuracy: 0.765800\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss:   0.067484 Validation Accuracy: 0.778400\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss:   0.059316 Validation Accuracy: 0.775000\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss:   0.069256 Validation Accuracy: 0.770800\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss:   0.092462 Validation Accuracy: 0.777200\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss:   0.110061 Validation Accuracy: 0.770000\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss:   0.087853 Validation Accuracy: 0.764400\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss:   0.069099 Validation Accuracy: 0.780200\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss:   0.071713 Validation Accuracy: 0.771800\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss:   0.080121 Validation Accuracy: 0.780200\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss:   0.142038 Validation Accuracy: 0.763000\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss:   0.088836 Validation Accuracy: 0.771400\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss:   0.081461 Validation Accuracy: 0.777400\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss:   0.067847 Validation Accuracy: 0.773400\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss:   0.089505 Validation Accuracy: 0.777600\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss:   0.098868 Validation Accuracy: 0.768400\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss:   0.087406 Validation Accuracy: 0.765600\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss:   0.071300 Validation Accuracy: 0.780800\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss:   0.065799 Validation Accuracy: 0.782400\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss:   0.067796 Validation Accuracy: 0.785400\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss:   0.113245 Validation Accuracy: 0.773400\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss:   0.064328 Validation Accuracy: 0.768600\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss:   0.065058 Validation Accuracy: 0.776400\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss:   0.094940 Validation Accuracy: 0.764600\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss:   0.083245 Validation Accuracy: 0.780600\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss:   0.116987 Validation Accuracy: 0.761400\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss:   0.061874 Validation Accuracy: 0.774000\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss:   0.059169 Validation Accuracy: 0.780200\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss:   0.054466 Validation Accuracy: 0.777000\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss:   0.080600 Validation Accuracy: 0.776000\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss:   0.138021 Validation Accuracy: 0.749400\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss:   0.059808 Validation Accuracy: 0.771600\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss:   0.064781 Validation Accuracy: 0.775400\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss:   0.060658 Validation Accuracy: 0.774000\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss:   0.079359 Validation Accuracy: 0.771800\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss:   0.110229 Validation Accuracy: 0.773800\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss:   0.071061 Validation Accuracy: 0.770800\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss:   0.054781 Validation Accuracy: 0.775800\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss:   0.054417 Validation Accuracy: 0.771800\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss:   0.084729 Validation Accuracy: 0.781200\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss:   0.091913 Validation Accuracy: 0.758000\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss:   0.067603 Validation Accuracy: 0.767400\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss:   0.079094 Validation Accuracy: 0.768600\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss:   0.063224 Validation Accuracy: 0.771800\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss:   0.083205 Validation Accuracy: 0.779000\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss:   0.096843 Validation Accuracy: 0.772200\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss:   0.052517 Validation Accuracy: 0.777600\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss:   0.068820 Validation Accuracy: 0.781400\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss:   0.066686 Validation Accuracy: 0.775600\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss:   0.061537 Validation Accuracy: 0.785000\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss:   0.085507 Validation Accuracy: 0.776800\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss:   0.062577 Validation Accuracy: 0.773000\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss:   0.063236 Validation Accuracy: 0.770400\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss:   0.062777 Validation Accuracy: 0.775000\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss:   0.086860 Validation Accuracy: 0.772000\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss:   0.081154 Validation Accuracy: 0.781800\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss:   0.060798 Validation Accuracy: 0.777600\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss:   0.053919 Validation Accuracy: 0.779800\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss:   0.060714 Validation Accuracy: 0.773000\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss:   0.064422 Validation Accuracy: 0.775000\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss:   0.080321 Validation Accuracy: 0.777400\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss:   0.061483 Validation Accuracy: 0.765800\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss:   0.057087 Validation Accuracy: 0.769000\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss:   0.052092 Validation Accuracy: 0.778400\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss:   0.079046 Validation Accuracy: 0.778000\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss:   0.080036 Validation Accuracy: 0.774400\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss:   0.063877 Validation Accuracy: 0.775400\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss:   0.055693 Validation Accuracy: 0.778800\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss:   0.060761 Validation Accuracy: 0.777600\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss:   0.080658 Validation Accuracy: 0.781600\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss:   0.095762 Validation Accuracy: 0.758400\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss:   0.058997 Validation Accuracy: 0.772600\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss:   0.055703 Validation Accuracy: 0.781600\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss:   0.052745 Validation Accuracy: 0.768600\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss:   0.083533 Validation Accuracy: 0.776600\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss:   0.085440 Validation Accuracy: 0.770400\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss:   0.058806 Validation Accuracy: 0.781200\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss:   0.054087 Validation Accuracy: 0.772400\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss:   0.051874 Validation Accuracy: 0.784200\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss:   0.066305 Validation Accuracy: 0.777600\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss:   0.107026 Validation Accuracy: 0.763400\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss:   0.049829 Validation Accuracy: 0.772800\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss:   0.052907 Validation Accuracy: 0.782800\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss:   0.052553 Validation Accuracy: 0.773800\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss:   0.068222 Validation Accuracy: 0.780800\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss:   0.056639 Validation Accuracy: 0.779400\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss:   0.049660 Validation Accuracy: 0.779200\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss:   0.066599 Validation Accuracy: 0.774400\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss:   0.049760 Validation Accuracy: 0.772800\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss:   0.068801 Validation Accuracy: 0.784000\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss:   0.108306 Validation Accuracy: 0.755600\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss:   0.048929 Validation Accuracy: 0.777000\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss:   0.056962 Validation Accuracy: 0.779200\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss:   0.059411 Validation Accuracy: 0.787800\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss:   0.073553 Validation Accuracy: 0.790000\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss:   0.088516 Validation Accuracy: 0.772400\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss:   0.056906 Validation Accuracy: 0.768600\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss:   0.039686 Validation Accuracy: 0.787600\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss:   0.059577 Validation Accuracy: 0.777600\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss:   0.064368 Validation Accuracy: 0.785800\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss:   0.072274 Validation Accuracy: 0.777600\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss:   0.048945 Validation Accuracy: 0.778800\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss:   0.051514 Validation Accuracy: 0.787600\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss:   0.067194 Validation Accuracy: 0.775200\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss:   0.068345 Validation Accuracy: 0.783000\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss:   0.076379 Validation Accuracy: 0.774800\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss:   0.060904 Validation Accuracy: 0.778000\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss:   0.042601 Validation Accuracy: 0.780200\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss:   0.045214 Validation Accuracy: 0.781000\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss:   0.078350 Validation Accuracy: 0.784400\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss:   0.065099 Validation Accuracy: 0.781200\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss:   0.050113 Validation Accuracy: 0.784600\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss:   0.051205 Validation Accuracy: 0.782800\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss:   0.069763 Validation Accuracy: 0.779400\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss:   0.063620 Validation Accuracy: 0.776200\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss:   0.076596 Validation Accuracy: 0.770200\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss:   0.053369 Validation Accuracy: 0.775400\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss:   0.041748 Validation Accuracy: 0.780600\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss:   0.051470 Validation Accuracy: 0.773400\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss:   0.079084 Validation Accuracy: 0.779800\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss:   0.068739 Validation Accuracy: 0.766600\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss:   0.060276 Validation Accuracy: 0.775600\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss:   0.061136 Validation Accuracy: 0.770400\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss:   0.051362 Validation Accuracy: 0.783800\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss:   0.063140 Validation Accuracy: 0.786400\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss:   0.074911 Validation Accuracy: 0.777000\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss:   0.059421 Validation Accuracy: 0.780000\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss:   0.051767 Validation Accuracy: 0.783600\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss:   0.052652 Validation Accuracy: 0.781200\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss:   0.067933 Validation Accuracy: 0.783400\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss:   0.091525 Validation Accuracy: 0.776200\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss:   0.043675 Validation Accuracy: 0.777800\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss:   0.056207 Validation Accuracy: 0.776800\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss:   0.051708 Validation Accuracy: 0.781600\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss:   0.059053 Validation Accuracy: 0.783400\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss:   0.076172 Validation Accuracy: 0.775600\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss:   0.046976 Validation Accuracy: 0.774000\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss:   0.051491 Validation Accuracy: 0.771000\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss:   0.047360 Validation Accuracy: 0.779600\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss:   0.079012 Validation Accuracy: 0.778200\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss:   0.069961 Validation Accuracy: 0.779600\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss:   0.065641 Validation Accuracy: 0.772200\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss:   0.044382 Validation Accuracy: 0.768600\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss:   0.041463 Validation Accuracy: 0.784400\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss:   0.063444 Validation Accuracy: 0.779600\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss:   0.068578 Validation Accuracy: 0.787400\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss:   0.053601 Validation Accuracy: 0.774400\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss:   0.043556 Validation Accuracy: 0.780800\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss:   0.045525 Validation Accuracy: 0.778800\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss:   0.052245 Validation Accuracy: 0.781000\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss:   0.083947 Validation Accuracy: 0.773600\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss:   0.046732 Validation Accuracy: 0.779000\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss:   0.037254 Validation Accuracy: 0.787600\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss:   0.036261 Validation Accuracy: 0.781600\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss:   0.052975 Validation Accuracy: 0.783200\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss:   0.059543 Validation Accuracy: 0.785400\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss:   0.046354 Validation Accuracy: 0.775000\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss:   0.047866 Validation Accuracy: 0.784600\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss:   0.049375 Validation Accuracy: 0.780600\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss:   0.059227 Validation Accuracy: 0.784800\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss:   0.051441 Validation Accuracy: 0.776600\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss:   0.060577 Validation Accuracy: 0.768800\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss:   0.054550 Validation Accuracy: 0.775800\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss:   0.041141 Validation Accuracy: 0.781200\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss:   0.057522 Validation Accuracy: 0.783000\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss:   0.073205 Validation Accuracy: 0.778600\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss:   0.060291 Validation Accuracy: 0.779000\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss:   0.037249 Validation Accuracy: 0.781800\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss:   0.046837 Validation Accuracy: 0.773800\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss:   0.056881 Validation Accuracy: 0.783600\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss:   0.046411 Validation Accuracy: 0.786000\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss:   0.048055 Validation Accuracy: 0.776600\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss:   0.033701 Validation Accuracy: 0.789600\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss:   0.035647 Validation Accuracy: 0.791600\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss:   0.061579 Validation Accuracy: 0.789000\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss:   0.053846 Validation Accuracy: 0.784600\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss:   0.048351 Validation Accuracy: 0.777600\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss:   0.057090 Validation Accuracy: 0.771200\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss:   0.048565 Validation Accuracy: 0.772000\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss:   0.076544 Validation Accuracy: 0.787000\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss:   0.060981 Validation Accuracy: 0.780800\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss:   0.044152 Validation Accuracy: 0.777000\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss:   0.042813 Validation Accuracy: 0.778000\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss:   0.034669 Validation Accuracy: 0.787800\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss:   0.051374 Validation Accuracy: 0.789800\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss:   0.049563 Validation Accuracy: 0.790800\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss:   0.045558 Validation Accuracy: 0.785000\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss:   0.042159 Validation Accuracy: 0.782200\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss:   0.050846 Validation Accuracy: 0.781000\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss:   0.061212 Validation Accuracy: 0.785600\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss:   0.062761 Validation Accuracy: 0.783000\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss:   0.041275 Validation Accuracy: 0.777800\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss:   0.037014 Validation Accuracy: 0.770000\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss:   0.049435 Validation Accuracy: 0.781000\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss:   0.043593 Validation Accuracy: 0.785600\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss:   0.050688 Validation Accuracy: 0.784800\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss:   0.032487 Validation Accuracy: 0.781000\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss:   0.029055 Validation Accuracy: 0.788400\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss:   0.035799 Validation Accuracy: 0.790400\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss:   0.060181 Validation Accuracy: 0.786800\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss:   0.068054 Validation Accuracy: 0.778000\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss:   0.044501 Validation Accuracy: 0.768800\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss:   0.038607 Validation Accuracy: 0.788400\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss:   0.039493 Validation Accuracy: 0.790400\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss:   0.051395 Validation Accuracy: 0.781400\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss:   0.048512 Validation Accuracy: 0.785400\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss:   0.047187 Validation Accuracy: 0.773200\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss:   0.045453 Validation Accuracy: 0.782200\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss:   0.053122 Validation Accuracy: 0.783600\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss:   0.045822 Validation Accuracy: 0.784600\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss:   0.051101 Validation Accuracy: 0.782000\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss:   0.042338 Validation Accuracy: 0.779200\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss:   0.047373 Validation Accuracy: 0.784600\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss:   0.053934 Validation Accuracy: 0.783600\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss:   0.044749 Validation Accuracy: 0.781400\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss:   0.045269 Validation Accuracy: 0.780200\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss:   0.033797 Validation Accuracy: 0.781000\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss:   0.038399 Validation Accuracy: 0.778800\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss:   0.044257 Validation Accuracy: 0.788800\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss:   0.057097 Validation Accuracy: 0.789000\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss:   0.031044 Validation Accuracy: 0.785600\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss:   0.037421 Validation Accuracy: 0.782000\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss:   0.030556 Validation Accuracy: 0.791600\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss:   0.040320 Validation Accuracy: 0.788000\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss:   0.049567 Validation Accuracy: 0.792400\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss:   0.066639 Validation Accuracy: 0.774600\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss:   0.033959 Validation Accuracy: 0.781400\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss:   0.038278 Validation Accuracy: 0.766400\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss:   0.034572 Validation Accuracy: 0.783800\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss:   0.048863 Validation Accuracy: 0.789800\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss:   0.037595 Validation Accuracy: 0.784000\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss:   0.046430 Validation Accuracy: 0.785800\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss:   0.060399 Validation Accuracy: 0.773600\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss:   0.041801 Validation Accuracy: 0.790800\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss:   0.044106 Validation Accuracy: 0.785000\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss:   0.036556 Validation Accuracy: 0.791000\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss:   0.033681 Validation Accuracy: 0.780400\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss:   0.030163 Validation Accuracy: 0.783800\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss:   0.039480 Validation Accuracy: 0.779000\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss:   0.049428 Validation Accuracy: 0.790600\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss:   0.040652 Validation Accuracy: 0.781800\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss:   0.041761 Validation Accuracy: 0.780800\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss:   0.040394 Validation Accuracy: 0.781800\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss:   0.048350 Validation Accuracy: 0.787600\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss:   0.047080 Validation Accuracy: 0.788800\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss:   0.040868 Validation Accuracy: 0.789200\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss:   0.032935 Validation Accuracy: 0.788000\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss:   0.029113 Validation Accuracy: 0.778000\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss:   0.038546 Validation Accuracy: 0.792600\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss:   0.038927 Validation Accuracy: 0.794200\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss:   0.043166 Validation Accuracy: 0.785000\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss:   0.036357 Validation Accuracy: 0.787400\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss:   0.036820 Validation Accuracy: 0.772200\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss:   0.067889 Validation Accuracy: 0.789800\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss:   0.042985 Validation Accuracy: 0.797200\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss:   0.045012 Validation Accuracy: 0.788400\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss:   0.041192 Validation Accuracy: 0.784200\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss:   0.028547 Validation Accuracy: 0.775800\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss:   0.046816 Validation Accuracy: 0.784200\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss:   0.049602 Validation Accuracy: 0.792600\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss:   0.036892 Validation Accuracy: 0.783400\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss:   0.035391 Validation Accuracy: 0.788800\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss:   0.042683 Validation Accuracy: 0.769200\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss:   0.037201 Validation Accuracy: 0.783000\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss:   0.052112 Validation Accuracy: 0.787200\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss:   0.038294 Validation Accuracy: 0.786400\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss:   0.036940 Validation Accuracy: 0.785600\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss:   0.028267 Validation Accuracy: 0.783600\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss:   0.034118 Validation Accuracy: 0.789400\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss:   0.039433 Validation Accuracy: 0.786600\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss:   0.045884 Validation Accuracy: 0.787800\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss:   0.033486 Validation Accuracy: 0.785000\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss:   0.026417 Validation Accuracy: 0.786800\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss:   0.030807 Validation Accuracy: 0.789800\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss:   0.036955 Validation Accuracy: 0.788000\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss:   0.036177 Validation Accuracy: 0.785000\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss:   0.035035 Validation Accuracy: 0.782400\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss:   0.027800 Validation Accuracy: 0.778800\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss:   0.031834 Validation Accuracy: 0.790000\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss:   0.035425 Validation Accuracy: 0.793200\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss:   0.047531 Validation Accuracy: 0.794800\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss:   0.031929 Validation Accuracy: 0.794400\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss:   0.023003 Validation Accuracy: 0.784000\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss:   0.044547 Validation Accuracy: 0.788800\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss:   0.042613 Validation Accuracy: 0.794400\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss:   0.039794 Validation Accuracy: 0.784800\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss:   0.034472 Validation Accuracy: 0.793400\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss:   0.032350 Validation Accuracy: 0.778800\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss:   0.030264 Validation Accuracy: 0.789800\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss:   0.034336 Validation Accuracy: 0.794600\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss:   0.034553 Validation Accuracy: 0.792200\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss:   0.043212 Validation Accuracy: 0.787600\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss:   0.029779 Validation Accuracy: 0.794400\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss:   0.026976 Validation Accuracy: 0.789200\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss:   0.034501 Validation Accuracy: 0.792400\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss:   0.031417 Validation Accuracy: 0.786200\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss:   0.035977 Validation Accuracy: 0.782600\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss:   0.033827 Validation Accuracy: 0.789800\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss:   0.033394 Validation Accuracy: 0.789200\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss:   0.039242 Validation Accuracy: 0.789800\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss:   0.029693 Validation Accuracy: 0.784400\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss:   0.033817 Validation Accuracy: 0.786200\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss:   0.035013 Validation Accuracy: 0.787000\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss:   0.026934 Validation Accuracy: 0.790000\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss:   0.032022 Validation Accuracy: 0.791200\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss:   0.048623 Validation Accuracy: 0.794000\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss:   0.035885 Validation Accuracy: 0.784800\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss:   0.024676 Validation Accuracy: 0.778000\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss:   0.028111 Validation Accuracy: 0.785800\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss:   0.030504 Validation Accuracy: 0.793600\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss:   0.025406 Validation Accuracy: 0.787400\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss:   0.034426 Validation Accuracy: 0.793800\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss:   0.027510 Validation Accuracy: 0.791800\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss:   0.036998 Validation Accuracy: 0.770800\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss:   0.037508 Validation Accuracy: 0.795400\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss:   0.051838 Validation Accuracy: 0.782400\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss:   0.039549 Validation Accuracy: 0.779200\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss:   0.022148 Validation Accuracy: 0.784800\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss:   0.019305 Validation Accuracy: 0.794800\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss:   0.040320 Validation Accuracy: 0.792000\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss:   0.038877 Validation Accuracy: 0.793600\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss:   0.029836 Validation Accuracy: 0.787200\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss:   0.029648 Validation Accuracy: 0.792600\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss:   0.023007 Validation Accuracy: 0.793200\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss:   0.035478 Validation Accuracy: 0.791800\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss:   0.029948 Validation Accuracy: 0.798400\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss:   0.035169 Validation Accuracy: 0.793600\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss:   0.027472 Validation Accuracy: 0.779600\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss:   0.029897 Validation Accuracy: 0.787200\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss:   0.045082 Validation Accuracy: 0.792000\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss:   0.029130 Validation Accuracy: 0.786400\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss:   0.031831 Validation Accuracy: 0.789000\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss:   0.018438 Validation Accuracy: 0.794200\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss:   0.031114 Validation Accuracy: 0.784400\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss:   0.049639 Validation Accuracy: 0.794600\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss:   0.023517 Validation Accuracy: 0.798600\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss:   0.034419 Validation Accuracy: 0.787400\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss:   0.031159 Validation Accuracy: 0.796800\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss:   0.024086 Validation Accuracy: 0.793200\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss:   0.035351 Validation Accuracy: 0.795800\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss:   0.031453 Validation Accuracy: 0.792200\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss:   0.027485 Validation Accuracy: 0.789600\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss:   0.027520 Validation Accuracy: 0.794400\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss:   0.018577 Validation Accuracy: 0.785200\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss:   0.029832 Validation Accuracy: 0.795600\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss:   0.030511 Validation Accuracy: 0.791600\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss:   0.037362 Validation Accuracy: 0.786400\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss:   0.018586 Validation Accuracy: 0.790000\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss:   0.017651 Validation Accuracy: 0.792200\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss:   0.031862 Validation Accuracy: 0.796000\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss:   0.035145 Validation Accuracy: 0.795400\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss:   0.032939 Validation Accuracy: 0.795600\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss:   0.025438 Validation Accuracy: 0.794000\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss:   0.029327 Validation Accuracy: 0.780600\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss:   0.033320 Validation Accuracy: 0.792200\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss:   0.030129 Validation Accuracy: 0.793200\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss:   0.040206 Validation Accuracy: 0.789400\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss:   0.026779 Validation Accuracy: 0.797200\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss:   0.027695 Validation Accuracy: 0.786600\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss:   0.032980 Validation Accuracy: 0.790400\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss:   0.022323 Validation Accuracy: 0.797800\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss:   0.029649 Validation Accuracy: 0.792000\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss:   0.017616 Validation Accuracy: 0.794600\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss:   0.028413 Validation Accuracy: 0.781400\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss:   0.044338 Validation Accuracy: 0.795200\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss:   0.032288 Validation Accuracy: 0.792800\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss:   0.029598 Validation Accuracy: 0.793400\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss:   0.025766 Validation Accuracy: 0.790000\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss:   0.029444 Validation Accuracy: 0.785000\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss:   0.034857 Validation Accuracy: 0.791200\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss:   0.028736 Validation Accuracy: 0.796200\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss:   0.022882 Validation Accuracy: 0.786600\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss:   0.027018 Validation Accuracy: 0.798800\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss:   0.023795 Validation Accuracy: 0.794800\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss:   0.035773 Validation Accuracy: 0.795800\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss:   0.022413 Validation Accuracy: 0.791800\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss:   0.027574 Validation Accuracy: 0.794800\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss:   0.022482 Validation Accuracy: 0.786400\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss:   0.028241 Validation Accuracy: 0.791400\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss:   0.024264 Validation Accuracy: 0.787600\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss:   0.020981 Validation Accuracy: 0.779200\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss:   0.028509 Validation Accuracy: 0.785200\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss:   0.018103 Validation Accuracy: 0.793800\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss:   0.017539 Validation Accuracy: 0.797600\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss:   0.027273 Validation Accuracy: 0.787000\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss:   0.026861 Validation Accuracy: 0.794800\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss:   0.025364 Validation Accuracy: 0.790800\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss:   0.021938 Validation Accuracy: 0.793000\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss:   0.025495 Validation Accuracy: 0.792200\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss:   0.025116 Validation Accuracy: 0.794800\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss:   0.026117 Validation Accuracy: 0.787600\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss:   0.031702 Validation Accuracy: 0.788800\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss:   0.015130 Validation Accuracy: 0.783400\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss:   0.017760 Validation Accuracy: 0.787600\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss:   0.023121 Validation Accuracy: 0.794400\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss:   0.032259 Validation Accuracy: 0.796600\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss:   0.029561 Validation Accuracy: 0.786600\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss:   0.021137 Validation Accuracy: 0.789200\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss:   0.024198 Validation Accuracy: 0.788600\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss:   0.030849 Validation Accuracy: 0.794400\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss:   0.030784 Validation Accuracy: 0.787000\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss:   0.026529 Validation Accuracy: 0.784000\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss:   0.023008 Validation Accuracy: 0.788200\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss:   0.024149 Validation Accuracy: 0.794200\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss:   0.023155 Validation Accuracy: 0.792000\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss:   0.032620 Validation Accuracy: 0.796400\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss:   0.025406 Validation Accuracy: 0.793200\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss:   0.015087 Validation Accuracy: 0.787000\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss:   0.031979 Validation Accuracy: 0.791400\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss:   0.034400 Validation Accuracy: 0.794400\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss:   0.029129 Validation Accuracy: 0.795800\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss:   0.024364 Validation Accuracy: 0.795200\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss:   0.025281 Validation Accuracy: 0.776600\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss:   0.022283 Validation Accuracy: 0.796400\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss:   0.024360 Validation Accuracy: 0.796600\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss:   0.025411 Validation Accuracy: 0.795000\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss:   0.022847 Validation Accuracy: 0.794200\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss:   0.027585 Validation Accuracy: 0.796200\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss:   0.022095 Validation Accuracy: 0.789000\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss:   0.026463 Validation Accuracy: 0.793800\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss:   0.025510 Validation Accuracy: 0.796800\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss:   0.047097 Validation Accuracy: 0.778600\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss:   0.027039 Validation Accuracy: 0.796800\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss:   0.041452 Validation Accuracy: 0.768400\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss:   0.035623 Validation Accuracy: 0.798600\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss:   0.027611 Validation Accuracy: 0.794800\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss:   0.025798 Validation Accuracy: 0.790200\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss:   0.016040 Validation Accuracy: 0.792600\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss:   0.036541 Validation Accuracy: 0.787800\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss:   0.021001 Validation Accuracy: 0.797800\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss:   0.018114 Validation Accuracy: 0.799200\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss:   0.024385 Validation Accuracy: 0.796200\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss:   0.021067 Validation Accuracy: 0.798200\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss:   0.014129 Validation Accuracy: 0.793800\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss:   0.023556 Validation Accuracy: 0.794800\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss:   0.018105 Validation Accuracy: 0.793000\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss:   0.025871 Validation Accuracy: 0.790400\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss:   0.018953 Validation Accuracy: 0.792800\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss:   0.034118 Validation Accuracy: 0.793000\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss:   0.025272 Validation Accuracy: 0.795400\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss:   0.021393 Validation Accuracy: 0.793400\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss:   0.024811 Validation Accuracy: 0.788200\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss:   0.019242 Validation Accuracy: 0.791400\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss:   0.021208 Validation Accuracy: 0.797200\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss:   0.023683 Validation Accuracy: 0.793800\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss:   0.024217 Validation Accuracy: 0.797400\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss:   0.021705 Validation Accuracy: 0.791400\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss:   0.012092 Validation Accuracy: 0.789000\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss:   0.014501 Validation Accuracy: 0.794000\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss:   0.033288 Validation Accuracy: 0.791800\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss:   0.019264 Validation Accuracy: 0.801200\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss:   0.021974 Validation Accuracy: 0.795400\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss:   0.019414 Validation Accuracy: 0.798200\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss:   0.020501 Validation Accuracy: 0.784600\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss:   0.025491 Validation Accuracy: 0.799000\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss:   0.014671 Validation Accuracy: 0.798400\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss:   0.019256 Validation Accuracy: 0.792200\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss:   0.011487 Validation Accuracy: 0.801200\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss:   0.020137 Validation Accuracy: 0.794800\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss:   0.027943 Validation Accuracy: 0.799600\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss:   0.018926 Validation Accuracy: 0.797200\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss:   0.019988 Validation Accuracy: 0.791800\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss:   0.014129 Validation Accuracy: 0.795200\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss:   0.024649 Validation Accuracy: 0.794800\n"
     ]
    }
   ],
   "source": [
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test our model against the test dataset.  This will be our final accuracy. we should have an accuracy greater than 50%. If we don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.78193359375\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAscAAAJ/CAYAAACUb342AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wdMdk+YPC5R4AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f79079c2ef0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 355
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
